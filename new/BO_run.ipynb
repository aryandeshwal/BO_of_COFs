{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "working-representation",
   "metadata": {},
   "source": [
    "# BO runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dental-elements",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In /home/cokes/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The text.latex.preview rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/cokes/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The mathtext.fallback_to_cm rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/cokes/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: Support for setting the 'mathtext.fallback_to_cm' rcParam is deprecated since 3.3 and will be removed two minor releases later; use 'mathtext.fallback : 'cm' instead.\n",
      "In /home/cokes/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The validate_bool_maybe_none function was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/cokes/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The savefig.jpeg_quality rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/cokes/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The keymap.all_axes rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/cokes/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The animation.avconv_path rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/cokes/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The animation.avconv_args rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from botorch.models import FixedNoiseGP, SingleTaskGP\n",
    "from gpytorch.kernels import ScaleKernel\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from botorch import fit_gpytorch_model\n",
    "from scipy.stats import norm\n",
    "from botorch.acquisition.analytic import ExpectedImprovement\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "descending-surveillance",
   "metadata": {},
   "source": [
    "load data from `prepare_Xy.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "alert-jackson",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69839"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pickle.load(open('inputs_and_outputs.pkl', 'rb'))['X']\n",
    "y = pickle.load(open('inputs_and_outputs.pkl', 'rb'))['y']\n",
    "y = np.reshape(y, (np.size(y), 1)) # for the GP\n",
    "nb_data = np.size(y)\n",
    "nb_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concerned-collection",
   "metadata": {},
   "source": [
    "convert to torch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "southwest-marshall",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.from_numpy(X)\n",
    "y = torch.from_numpy(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "statutory-offset",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([69839, 12])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "refined-conspiracy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([69839, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fifth-equivalent",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_unsqueezed = X.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "durable-columbus",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69839"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "available-island",
   "metadata": {},
   "source": [
    "`ids_acquired[r, i]` will give ID of COF acquired during iteration `i` from run `r`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "pursuant-decision",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb_iterations: # of COFs to acquire = # of iterations in BO\n",
    "# nb_COFs_initialization: # of COFs to acquire at random to initiate the GP and BO.\n",
    "def bo_run(nb_iterations, nb_COFs_initialization, which_acquisition, verbose=False):\n",
    "    assert nb_iterations > nb_COFs_initialization\n",
    "    \n",
    "    # select initial COFs for training data randomly.\n",
    "    # idea is to keep populating this ids_acquired and return it for analysis.\n",
    "    ids_acquired = np.random.choice(np.arange((nb_data)), size=nb_COFs_initialization, replace=False)\n",
    "    \n",
    "    explore_exploit_balance = np.array([(np.NaN, np.NaN) for i in range(nb_iterations)])\n",
    "    if which_acquisition != \"EI\":\n",
    "        explore_exploit_balance = [] # don't bother storing NaNs\n",
    "\n",
    "    # initialize acquired y, since it requires normalization\n",
    "    y_acquired = y[ids_acquired]\n",
    "    # standardize outputs\n",
    "    y_acquired = (y_acquired - torch.mean(y_acquired)) / torch.std(y_acquired)\n",
    "    \n",
    "    for i in range(nb_COFs_initialization, nb_iterations):\n",
    "        print(\"iteration:\", i, end=\"\\r\")\n",
    "        # construct and fit GP model\n",
    "        model = SingleTaskGP(X[ids_acquired, :], y_acquired)\n",
    "        mll = ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "        fit_gpytorch_model(mll)\n",
    "\n",
    "        # set up acquisition function\n",
    "        if which_acquisition == \"EI\":\n",
    "            acquisition_function = ExpectedImprovement(model, best_f=y_acquired.max().item())\n",
    "            \n",
    "            # compute aquisition function at each COF in the database. \n",
    "#             batch_size = 35000 # need to do in batches to avoid mem issues\n",
    "#             acquisition_values = torch.zeros((nb_data))\n",
    "#             acquisition_values[:] = np.NaN # for safety\n",
    "#             nb_batches = nb_data // batch_size\n",
    "#             for ba in range(nb_batches+1):\n",
    "#                 id_start = ba * batch_size\n",
    "#                 id_end   = id_start + batch_size\n",
    "#                 if id_end > nb_data:\n",
    "#                     id_end = nb_data\n",
    "#                 with torch.no_grad():\n",
    "#                     acquisition_values[id_start:id_end] = acquisition_function.forward(X_unsqueezed[id_start:id_end])\n",
    "#             assert acquisition_values.isnan().sum().item() == 0 # so that all are filled properly.\n",
    "            with torch.no_grad(): # to avoid memory issues; we arent using the gradient...\n",
    "                acquisition_values = acquisition_function.forward(X_unsqueezed) # runs out of memory\n",
    "        elif which_acquisition == \"max y_hat\":\n",
    "            with torch.no_grad():\n",
    "                acquisition_values = model.posterior(X_unsqueezed).mean.squeeze()\n",
    "        elif which_acquisition == \"max sigma\":\n",
    "            with torch.no_grad():\n",
    "                acquisition_values = model.posterior(X_unsqueezed).variance.squeeze()\n",
    "        else:\n",
    "            raise Exception(\"not a valid acquisition function\")\n",
    "\n",
    "        # select COF to acquire with maximal aquisition value, which is not in the acquired set already\n",
    "        ids_sorted_by_aquisition = acquisition_values.argsort(descending=True)\n",
    "        for id_max_aquisition_all in ids_sorted_by_aquisition:\n",
    "            if not id_max_aquisition_all.item() in ids_acquired:\n",
    "                id_max_aquisition = id_max_aquisition_all.item()\n",
    "                break\n",
    "\n",
    "        # acquire this COF\n",
    "        ids_acquired = np.concatenate((ids_acquired, [id_max_aquisition]))\n",
    "        assert np.size(ids_acquired) == i + 1\n",
    "        \n",
    "        # if EI, compute explore-exploit component.\n",
    "        if which_acquisition == \"EI\":\n",
    "            y_pred = model.posterior(X_unsqueezed[id_max_aquisition]).mean.squeeze().detach().numpy()\n",
    "            sigma_pred = np.sqrt(model.posterior(X_unsqueezed[id_max_aquisition]).variance.squeeze().detach().numpy())\n",
    "            \n",
    "            y_max = y_acquired.max().item()\n",
    "            \n",
    "            z = (y_pred - y_max) / sigma_pred\n",
    "            explore_term = sigma_pred * norm.pdf(z)\n",
    "            exploit_term = (y_pred - y_max) * norm.cdf(z)\n",
    "            \n",
    "            assert np.isclose(explore_term + exploit_term, acquisition_values[id_max_aquisition].item())\n",
    "            \n",
    "#             print(\"explore term\", explore_term)\n",
    "#             print(\"exploit term\", exploit_term)\n",
    "#             print(\"my EI\", explore_term + exploit_term)\n",
    "#             print(\"auto EI\", acquisition_values[id_max_aquisition].item())\n",
    "\n",
    "            explore_exploit_balance[i] = (explore_term, exploit_term)\n",
    "\n",
    "        # update y aquired; start over to normalize properly\n",
    "        y_acquired = y[ids_acquired] # start over to normalize y properly\n",
    "        y_acquired = (y_acquired - torch.mean(y_acquired)) / torch.std(y_acquired)\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"\\tacquired COF\", id_max_aquisition, \"with y = \", y[id_max_aquisition].item())\n",
    "            print(\"\\tbest y acquired:\", y[ids_acquired].max().item())\n",
    "        \n",
    "    assert np.size(ids_acquired) == nb_iterations\n",
    "    return ids_acquired, explore_exploit_balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "younger-maker",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# COFs in initialization: 5\n",
      "\n",
      "\n",
      "RUN 0\n",
      "took time t =  0.842117973168691 min\n",
      "# COFs in initialization: 10\n",
      "\n",
      "\n",
      "RUN 0\n",
      "took time t =  1.011927890777588 min\n",
      "# COFs in initialization: 15\n",
      "\n",
      "\n",
      "RUN 0\n",
      "took time t =  0.43527604738871256 min\n"
     ]
    }
   ],
   "source": [
    "which_acquisition = \"EI\"\n",
    "# which_acquisition = \"max y_hat\"\n",
    "# which_acquisition = \"max sigma\"\n",
    "nb_iterations = 20\n",
    "nb_runs = 1\n",
    "nb_COFs_initializations = {\"EI\": [5, 10, 15], \"max y_hat\": [10], \"max sigma\": [10]}\n",
    "\n",
    "for nb_COFs_initialization in nb_COFs_initializations[which_acquisition]:\n",
    "    print(\"# COFs in initialization:\", nb_COFs_initialization)\n",
    "    # store results here.\n",
    "    bo_res = dict() \n",
    "    bo_res['nb_runs']                 = nb_runs\n",
    "    bo_res['nb_iterations']           = nb_iterations\n",
    "    bo_res['ids_acquired']            = []\n",
    "    bo_res['explore_exploit_balance'] = []\n",
    "    \n",
    "    for r in range(nb_runs):\n",
    "        print(\"\\nRUN\", r)\n",
    "        t0 = time.time()\n",
    "        \n",
    "        ids_acquired, explore_exploit_balance = bo_run(bo_res['nb_iterations'], nb_COFs_initialization, which_acquisition)\n",
    "        \n",
    "        # store results from this run.\n",
    "        bo_res['ids_acquired'].append(ids_acquired)\n",
    "        bo_res['explore_exploit_balance'].append(explore_exploit_balance)\n",
    "        \n",
    "        print(\"took time t = \", (time.time() - t0) / 60, \"min\\n\")\n",
    "    \n",
    "    # save results from all runs\n",
    "    with open('bo_results_' + which_acquisition + \"_initiate_with_{0}\".format(nb_COFs_initialization) + '.pkl', 'wb') as file:\n",
    "        pickle.dump(bo_res, file)\n",
    "        \n",
    "with open('bo_results_nb_COF_initializations.pkl', 'wb') as file:\n",
    "    pickle.dump(nb_COFs_initializations, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loose-virgin",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
